{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from mol2vec.features import mol2alt_sentence, mol2sentence, MolSentence, DfVec, sentences2vec\n",
    "from gensim.models import word2vec\n",
    "from rdkit import Chem \n",
    "from rdkit.Chem import Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pretrain_features = pd.read_csv('pretrain_features.csv')\n",
    "df_pretrain_labels = pd.read_csv('pretrain_labels.csv')\n",
    "df_train_features = pd.read_csv('train_features.csv')\n",
    "df_train_labels = pd.read_csv('train_labels.csv')\n",
    "df_test_features = pd.read_csv('test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the features from the dataframe \n",
    "pretrain_features = df_pretrain_features.iloc[:,2:]\n",
    "train_features = df_train_features.iloc[:,2:]\n",
    "test_features = df_test_features.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Feature creation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1.1) feature compression from given features (RDKit) with an autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 17:16:11.831370: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-31 17:16:11.831482: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-05-31 17:16:12.005759: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-05-31 17:16:12.005977: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   7/1250 [..............................] - ETA: 10s - loss: 0.0533 - accuracy: 0.9526 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 17:16:12.173583: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1248/1250 [============================>.] - ETA: 0s - loss: 0.0149 - accuracy: 0.9864"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 17:16:20.276340: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.0149 - accuracy: 0.9864 - val_loss: 0.0129 - val_accuracy: 0.9885\n",
      "Epoch 2/5\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.0126 - accuracy: 0.9886 - val_loss: 0.0124 - val_accuracy: 0.9888\n",
      "Epoch 3/5\n",
      "1250/1250 [==============================] - 8s 7ms/step - loss: 0.0122 - accuracy: 0.9887 - val_loss: 0.0120 - val_accuracy: 0.9888\n",
      "Epoch 4/5\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.0120 - accuracy: 0.9888 - val_loss: 0.0119 - val_accuracy: 0.9887\n",
      "Epoch 5/5\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.0118 - accuracy: 0.9888 - val_loss: 0.0117 - val_accuracy: 0.9889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 17:16:55.699597: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "#training test split\n",
    "X_train, X_test = train_test_split(pretrain_features, test_size=0.2, random_state=42)\n",
    "\n",
    "#train the autoencoder on the pretrain set\n",
    "autoencoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1000, activation='tanh'),\n",
    "    tf.keras.layers.Dense(500, activation='tanh'),\n",
    "    tf.keras.layers.Dense(250, activation='tanh'),\n",
    "    tf.keras.layers.Dense(50, activation='tanh', name = 'bottelneck'),\n",
    "    tf.keras.layers.Dense(250, activation='tanh'),\n",
    "    tf.keras.layers.Dense(500, activation='tanh'),\n",
    "    tf.keras.layers.Dense(1000, activation='tanh'),\n",
    "])\n",
    "\n",
    "#compile the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='mse',\n",
    "                    metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy')])\n",
    "\n",
    "#train the autoencoder\n",
    "history = autoencoder.fit(X_train, X_train, epochs=5, validation_data = (X_test, X_test))\n",
    "\n",
    "#build model to extract the bottelneck compressed features \n",
    "layer_name = 'bottelneck'\n",
    "compress_model = Model(inputs=autoencoder.input,\n",
    "                                 outputs=autoencoder.get_layer(layer_name).output)\n",
    "\n",
    "#compress the features using the autoencoder \n",
    "compressed_pretrain_features = pd.DataFrame(compress_model.predict(pretrain_features))\n",
    "compressed_train_features = pd.DataFrame(compress_model.predict(train_features))\n",
    "compressed_test_features = pd.DataFrame(compress_model.predict(test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1.2) additional feature extraction using mol2vec\n",
    "following: https://www.kaggle.com/code/vladislavkisin/tutorial-ml-in-chemistry-research-rdkit-mol2vec/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the smiles strings\n",
    "pretrain = pd.DataFrame(df_pretrain_features['smiles'])\n",
    "train = pd.DataFrame(df_train_features['smiles'])\n",
    "test = pd.DataFrame(df_test_features['smiles'])\n",
    "\n",
    "#Transforming SMILES to MOL\n",
    "pretrain['mol'] = pretrain['smiles'].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "train['mol'] = train['smiles'].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "test['mol'] = test['smiles'].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "\n",
    "#Loading pre-trained model via word2vec\n",
    "model = word2vec.Word2Vec.load('model_300dim.pkl')\n",
    "\n",
    "#Constructing sentences\n",
    "pretrain['sentence'] = pretrain.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], 1)), axis=1)\n",
    "train['sentence'] = train.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], 1)), axis=1)\n",
    "test['sentence'] = test.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], 1)), axis=1)\n",
    "\n",
    "#Extracting embeddings to a numpy.array\n",
    "#Note that we always should mark unseen='UNK' in sentence2vec() so that model is taught how to handle unknown substructures\n",
    "pretrain['mol2vec'] = [DfVec(x) for x in sentences2vec(pretrain['sentence'], model, unseen='UNK')]\n",
    "train['mol2vec'] = [DfVec(x) for x in sentences2vec(train['sentence'], model, unseen='UNK')]\n",
    "test['mol2vec'] = [DfVec(x) for x in sentences2vec(test['sentence'], model, unseen='UNK')]\n",
    "\n",
    "X_pretrain = np.array([x.vec for x in pretrain['mol2vec']])\n",
    "X_train = np.array([x.vec for x in train['mol2vec']])\n",
    "X_test = np.array([x.vec for x in test['mol2vec']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store features in dataframe\n",
    "vec_pretrain_features= pd.DataFrame(X_pretrain)\n",
    "vec_train_features = pd.DataFrame(X_train)\n",
    "vec_test_features= pd.DataFrame(X_test)\n",
    "\n",
    "#concat features from 1.1 and 1.2\n",
    "final_pretrain_features = pd.concat([compressed_pretrain_features, vec_pretrain_features], axis = 1)\n",
    "final_train_features = pd.concat([compressed_train_features, vec_train_features], axis = 1)\n",
    "final_test_features = pd.concat([compressed_test_features, vec_test_features], axis = 1)\n",
    "\n",
    "#save the final features\n",
    "final_pretrain_features.to_csv('pretrain_features_new.csv', index=False)\n",
    "final_train_features.to_csv('train_features_new.csv', index=False)\n",
    "final_test_features.to_csv('test_features_new.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
