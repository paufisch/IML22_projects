{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import zscore\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data sets\n",
    "df_train = pd.read_csv('train_features.csv')\n",
    "df_train_labels = pd.read_csv('train_labels.csv')\n",
    "df_test = pd.read_csv('test_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join time-data for patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for a given pit number this function calculates features for each row in the given dataframe and returns a long dataframe with all the features \n",
    "\"\"\"\n",
    "\n",
    "def feature_creation(pid,df):\n",
    "    \n",
    "    patient = df.loc[df['pid'] == pid]\n",
    "\n",
    "    #second store average into a new dataframe\n",
    "    mean = pd.DataFrame(patient.mean()).transpose()#mean for each feature\n",
    "    mean = mean.drop('Time',axis = 1)#delete time from mean_1\n",
    "\n",
    "    #add new features\n",
    "    maxi = pd.DataFrame(patient.iloc[:,3:].max()).transpose()#maximum\n",
    "    mini = pd.DataFrame(patient.iloc[:,3:].min()).transpose()#maximum\n",
    "    kurt = pd.DataFrame(patient.iloc[:,3:].kurtosis()).transpose()#kurtosis\n",
    "    mad = pd.DataFrame(patient.iloc[:,3:].mad()).transpose()#mean absolute deviation\n",
    "    skew = pd.DataFrame(patient.iloc[:,3:].skew()).transpose()#skewedness\n",
    "    std = pd.DataFrame(patient.iloc[:,3:].std()).transpose()#standard deviation\n",
    "    nans = pd.DataFrame(mean.isnull().values.astype(int),columns = mean.isnull().columns)#1 if all entreis are 0 else 0\n",
    "    \n",
    "    #concatenate all features\n",
    "    total = pd.concat([mean,maxi,mini,kurt,mad,skew,std,nans], axis = 1, ignore_index = True)\n",
    "    return total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 18994/18994 [01:23<00:00, 228.28it/s]\n"
     ]
    }
   ],
   "source": [
    "#extract the unique pid numbers in the correct order\n",
    "_, idx = np.unique(df_train.values[:, 0], return_index=True)\n",
    "pids = df_train.values[:, 0]\n",
    "pids = pids[np.sort(idx)]\n",
    "\n",
    "#initialize final dataframe\n",
    "df_all = feature_creation(pids[0],df_train)\n",
    "\n",
    "#loop over all patient pids\n",
    "for i in tqdm(range(1,pids.size)):\n",
    "    #for each pid call the function feature creation\n",
    "    new_data = feature_creation(pids[i],df_train)\n",
    "    #concatenate the rows for all patients to a new dataframe\n",
    "    df_all = pd.concat([df_all,new_data], axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 12663/12663 [00:51<00:00, 244.86it/s]\n"
     ]
    }
   ],
   "source": [
    "#do the same operations as in the cell above for the training set!\n",
    "_, idx2 = np.unique(df_test.values[:, 0], return_index=True)\n",
    "pids_2 = df_test.values[:, 0]\n",
    "pids_2 = pids_2[np.sort(idx2)]\n",
    "df_all_test = feature_creation(pids_2[0],df_test)\n",
    "\n",
    "for i in tqdm(range(1,pids_2.size)):\n",
    "    new_data_2 = feature_creation(pids_2[i],df_test)\n",
    "    df_all_test = pd.concat([df_all_test,new_data_2], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "In order to preprocess the data we want to take the follwing steps\n",
    "\n",
    "1. **fix missing values**\n",
    "    - replace missing values by the mean of the data in this row \n",
    "    - note that the missing values could also be replaced by other measueres such as the median\n",
    "  \n",
    "  \n",
    "2. **standardize data**\n",
    "    - fix heavy tailed distribution\n",
    "    - distribute arround 0 with standard deviation 1\n",
    "    - we can also additionaly normalize the data which is usefull in some cases(also in ours?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints the percentage of missing values for each row of the data frame\n",
    "#print(df_all.isna().sum()/len(df_all)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this loops over the columns names\n",
    "\"\"\"\n",
    "#Find the missing percentage of each column in the training set.\n",
    "def find_missing_percent(data):\n",
    "    \n",
    "    #Returns dataframe containing the total missing values and percentage of total\n",
    "    #missing values of a column.\n",
    "    \n",
    "    miss_df = pd.DataFrame({'ColumnName':[],'TotalMissingVals':[],'PercentMissing':[]})\n",
    "    for col in data.columns:\n",
    "        sum_miss_val = data[col].isna().sum()\n",
    "        percent_miss_val = round((sum_miss_val/data.shape[0])*100,2)\n",
    "        miss_df = pd.concat([miss_df,pd.DataFrame({'ColumnName': [col],'TotalMissingVals': [sum_miss_val],'PercentMissing': [percent_miss_val]})])\n",
    "    return miss_df\n",
    "\n",
    "miss_df = find_missing_percent(df_train)\n",
    "\"\"\"\n",
    "    \n",
    "#Find the missing percentage of each column in the training set.\n",
    "#we cannot reference columns by name as we have multiple columns named the same\n",
    "def find_missing_percent(data):\n",
    "    \"\"\"\n",
    "    Returns dataframe containing the total missing values and percentage of total\n",
    "    missing values of a column.\n",
    "    \"\"\"\n",
    "    miss_df = pd.DataFrame({'ColumnIndex':[],'TotalMissingVals':[],'PercentMissing':[]})\n",
    "    for i in range(data.shape[1]):\n",
    "        sum_miss_val = data.iloc[:,i].isna().sum()\n",
    "        percent_miss_val = round((sum_miss_val/data.shape[0])*100,2)\n",
    "        miss_df = pd.concat([miss_df,pd.DataFrame({'ColumnIndex': [int(i)],'TotalMissingVals': [sum_miss_val],'PercentMissing': [percent_miss_val]})])\n",
    "    return miss_df\n",
    "\n",
    "miss_df = find_missing_percent(df_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#drops all the columns that have more than 70% missing values\n",
    "#and do the same for the test set\n",
    "\"\"\"\n",
    "make sure that this actually drops the right columns as we have different columns named the same!\n",
    "\"\"\"\n",
    "drop_cols = miss_df[miss_df['PercentMissing'] > 70].ColumnIndex\n",
    "drop_cols = np.array(drop_cols).astype(int)\n",
    "df_all = df_all.drop(df_all.columns[drop_cols],axis=1)\n",
    "df_all_test = df_all_test.drop(df_all_test.columns[drop_cols],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "we could also try to use other methods to replace the empty entries for example the k-nearest neighbour method\n",
    "\"\"\"\n",
    "#fills all empty entries with the average value for that column:\n",
    "df_all = df_all.fillna(df_all.mean()) #maybe try median\n",
    "#should we do this as well for the df_test???\n",
    "df_all_test = df_all_test.fillna(df_all_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Standardization\n",
    "for the moment I just standardize all rows to be distributed arround 0 with std deviation 1 (not caring about the distribution of the data)\n",
    "\n",
    "It could be necessery to fix the distribution\n",
    " - fixing skewdnedd using a log transformation\n",
    " - removing outliers by observing the z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z-score scaling:\n",
    "# create a scaler object\n",
    "std_scaler = StandardScaler()\n",
    "# fit and transform the data\n",
    "df_all = pd.DataFrame(std_scaler.fit_transform(df_all), columns=df_all.columns)\n",
    "df_all_test = pd.DataFrame(std_scaler.fit_transform(df_all_test), columns=df_all_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**now we are ready to start the classification tasks!**\n",
    "- **df_all is the training set,**\n",
    "- **df_train_labels are the corresponding labels**\n",
    "- **and df_all_test is the test set**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUB-TASK 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** anticipate the further needs of the patient \n",
    "- binary classification (0: no further tests, 1: further tests)\n",
    "- labels we want to predict:\n",
    "    - LABEL_BaseExcess, LABEL_Fibrinogen, LABEL_AST, LABEL_Alkalinephos, LABEL_Bilirubin_total, LABEL_Lactate, LABEL_TroponinI, LABEL_SaO2, LABEL_Bilirubin_direct, LABEL_EtCO2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the labels into the right format for training:\n",
    "#extract the labels from df_train_labels in which we are interested\n",
    "labels_task1 = df_train_labels[['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST', 'LABEL_Alkalinephos', 'LABEL_Bilirubin_total', 'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2', 'LABEL_Bilirubin_direct', 'LABEL_EtCO2']]\n",
    "#extract X and y as np.arrays from the dataframe\n",
    "X = df_all.to_numpy()[:,1:]\n",
    "Y = labels_task1.to_numpy()\n",
    "#test_set\n",
    "X_test = df_all_test.to_numpy()[:,1:]\n",
    "#train-test-split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for i = 0:  0.8794736842105263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█████                                        | 1/9 [00:07<00:57,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for i = 1:  0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██████████                                   | 2/9 [00:14<00:49,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for i = 2:  0.7752631578947369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███████████████                              | 3/9 [00:21<00:42,  7.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for i = 3:  0.7694736842105263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████████████████████                         | 4/9 [00:28<00:35,  7.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for i = 4:  0.7731578947368422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████████████████████████                    | 5/9 [00:34<00:27,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for i = 5:  0.8326315789473684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████████████████████████████               | 6/9 [00:40<00:19,  6.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for i = 6:  0.9157894736842105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████████████████████████████████          | 7/9 [00:46<00:12,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for i = 7:  0.8215789473684211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████████████████████████████████████     | 8/9 [00:54<00:06,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for i = 8:  0.9689473684210527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 9/9 [00:59<00:00,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for i = 9:  0.9536842105263158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "#SVC for label 1 in the list of labels\n",
    "#create probabilistic predictions \n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(x_train,y_train[:,0])\n",
    "y_pred = clf.predict(x_test)\n",
    "print('accuracy for i = 0: ',accuracy_score(y_test[:,0],y_pred))\n",
    "prob = clf.predict_proba(x_test)[:,1].reshape(-1,1)#probability to get a 1\n",
    "\n",
    "for i in tqdm(range(1,10)):   \n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(x_train,y_train[:,i])\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(f'accuracy for i = {i}: ',accuracy_score(y_test[:,i],y_pred))\n",
    "    prob_i = clf.predict_proba(x_test)[:,1].reshape(-1,1)#probability to get a 1\n",
    "    prob = np.concatenate((prob,prob_i),axis=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 9/9 [01:07<00:00,  7.47s/it]\n"
     ]
    }
   ],
   "source": [
    "#now that we have tested the accuracy of our model we can now train the final model using the whole data set\n",
    "clf_1 = RandomForestClassifier()#Random Forest Classifier\n",
    "clf_1.fit(X,Y[:,0])\n",
    "pred = clf_1.predict_proba(X_test)[:,1].reshape(-1,1)#probability to get a 1\n",
    "\n",
    "for i in tqdm(range(1,10)):   \n",
    "    clf_1 = RandomForestClassifier()\n",
    "    clf_1.fit(X,Y[:,i])\n",
    "    pred_i = clf_1.predict_proba(X_test)[:,1].reshape(-1,1)#probability to get a 1\n",
    "    pred = np.concatenate((pred,pred_i),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put this into a dataframe\n",
    "df_task1 = pd.DataFrame(pred,columns= ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST', 'LABEL_Alkalinephos', 'LABEL_Bilirubin_total', 'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2', 'LABEL_Bilirubin_direct', 'LABEL_EtCO2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUB-TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the labels from df_train_labels in which we are interested\n",
    "labels_task2 = df_train_labels[['LABEL_Sepsis']]\n",
    "#extract X and y as np.arrays from the dataframe\n",
    "Y2 = labels_task2.to_numpy().reshape(-1,1)\n",
    "#train-test-split\n",
    "x2_train, x2_test, y2_train, y2_test = train_test_split(X, Y2, train_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  0.9457894736842105\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "#create probabilistic predictions \n",
    "#clf2 = svm.SVC(probability = True)#kernel='rbf'\n",
    "clf2 = RandomForestClassifier()\n",
    "clf2.fit(x2_train,y2_train.reshape(-1))\n",
    "y2_pred = clf2.predict(x2_test)\n",
    "print('accuracy : ',accuracy_score(y2_test,y2_pred))\n",
    "prob2 = clf2.predict_proba(x2_test)[:,1].reshape(-1,1)#probability to get a 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we have tested the accuracy of our model we can now train the final model using the whole data set\n",
    "#clf2 = svm.SVC(probability = True)#kernel='rbf'\n",
    "clf_2 = RandomForestClassifier()\n",
    "clf_2.fit(X,Y2.reshape(-1))\n",
    "prediction_2 = clf_2.predict_proba(X_test)[:,1].reshape(-1,1)#probability to get a 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12664, 10)\n",
      "(12664, 1)\n"
     ]
    }
   ],
   "source": [
    "df_task2 = pd.DataFrame(prediction_2,columns= ['LABEL_Sepsis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUB-TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#extract the labels from df_train_labels in which we are interested\n",
    "labels_task3 = df_train_labels[['LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate']]\n",
    "#extract X and y as np.arrays from the dataframe\n",
    "Y3 = labels_task3.to_numpy()\n",
    "\n",
    "#train-test-split\n",
    "x3_train, x3_test, y3_train, y3_test = train_test_split(X, Y3, train_size=0.9)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3902456224039975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████                              | 1/3 [01:12<02:24, 72.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6064882875616302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████████████████████████████               | 2/3 [02:31<01:16, 76.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3661576010486117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [03:45<00:00, 75.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6523939058597674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#test_scaled = StandardScaler().fit_transform(x3_test)\n",
    "reg = RandomForestRegressor(n_estimators=100, random_state=420)\n",
    "reg.fit(x3_train, y3_train[:,0])#.ravel())\n",
    "print(reg.score(x3_test, y3_test[:,0]))\n",
    "# predict on test set\n",
    "predictions_3 = reg.predict(x3_test).reshape(-1,1)\n",
    "\n",
    "for i in tqdm(range(1,4)):\n",
    "    \n",
    "    reg = RandomForestRegressor(n_estimators=100, random_state=420)\n",
    "    reg.fit(x3_train, y3_train[:,i])#.ravel())\n",
    "    print(reg.score(x3_test, y3_test[:,i]))\n",
    "    # predict on test set\n",
    "    predictions_3i = reg.predict(x3_test).reshape(-1,1)\n",
    "    predictions_3 = np.concatenate((predictions_3,predictions_3i),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [04:19<00:00, 86.35s/it]\n"
     ]
    }
   ],
   "source": [
    "#test_scaled_4 = StandardScaler().fit_transform(X_test)\n",
    "reg4 = RandomForestRegressor(n_estimators=100, random_state=420)\n",
    "reg4.fit(X, Y3[:,0])#.ravel())\n",
    "# predict on test set\n",
    "predictions_4 = reg.predict(X_test).reshape(-1,1)\n",
    "\n",
    "for i in tqdm(range(1,4)):\n",
    "    \n",
    "    reg4 = RandomForestRegressor(n_estimators=100, random_state=420)\n",
    "    reg4.fit(X, Y3[:,i])#.ravel())\n",
    "    # predict on test set\n",
    "    predictions_4i = reg4.predict(X_test).reshape(-1,1)\n",
    "    predictions_4 = np.concatenate((predictions_4,predictions_4i),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task3 = pd.DataFrame(predictions_4,columns= ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_labels = pd.DataFrame(pids_2.astype(int), columns = ['pid'])\n",
    "df_handin = pd.concat([pid_labels,df_task1,df_task2,df_task3],axis=1)\n",
    "df_handin.to_csv('prediction_sorted.csv', index=False, float_format='%.3f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
